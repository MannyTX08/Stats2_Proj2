# The passenger departed from S and had Pclass = 3, lets find an appropriate value
# Replace missing fare value with median fare for class/embarkment
nullFare = full[is.na(full$Fare),] #1044
ggplot(full[full$Pclass == '3' & full$Embarked == 'S', ], aes(x = Fare)) +
geom_density(fill = '#99d6ff', alpha=0.4) +
geom_vline(aes(xintercept=median(Fare, na.rm=T)),colour='red', linetype='dashed', lwd=1) +
scale_x_continuous(breaks = seq(0,70,10), labels=dollar_format()) +
ggtitle("Density of Ticket Fare for Embarked = S and Pclass = 3",
subtitle = paste0("Median Fare: $",
median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm=T))) +
theme_few()
full$Fare[1044] <- median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE) # $8.05
# Using mice package impute values for Age that are missing
sum(is.na(train$Age)) # 177 missing values
sum(is.na(full$Age))  # 263 missing values in both train and test
# Create Age as a categorical variable
#   Be sure to run this BEFORE imputing with mice and rf
full$AgeBin<-addNA(cut(full$Age, seq(0, 90, by=10)))
#full$AgeBin[10:35]
l<-levels(full$AgeBin)[-(length(levels(full$AgeBin)))]
#l
#    replace <NA> with 'unknown'
levels(full$AgeBin)<-c(l, 'unknown')
full$AgeBin[10:35]
# Create a family = siblings + parents/children
# -Possibly for dimension reducing
full$Family = full$Parch + full$SibSp
# Make variables factors into factors
factor_vars <- c('PassengerId','Pclass','Sex','Embarked','Title', 'AgeBin')
full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))
# Set a random seed
set.seed(129)
# Perform mice imputation, excluding certain less-than-useful variables:
mice_mod <- mice(full[, !names(full) %in% c('PassengerId','Name','Ticket','Cabin','Survived')], method='rf')
# Save the complete output
mice_output <- complete(mice_mod)
# Plot age distributions of raw data against imputed from mice package
par(mfrow=c(1,2))
hist(full$Age, freq=F, main='Age: Original Data', col='lightblue', ylim=c(0,0.04))
hist(mice_output$Age, freq=F, main='Age: MICE Imputation Output', col='lightblue', ylim=c(0,0.04))
# Replace Age variable from the mice model
full$Age <- mice_output$Age
# Show new number of missing Age values is now 0
sum(is.na(full$Age))
# Create DF of independent/dependent variables
nonvars = c("PassengerId","Name","Ticket","Cabin")
full2 = full[,!(names(full) %in% nonvars)]
str(full2)
convert.vars <- c('Pclass','Sex','Embarked','Title', 'AgeBin')
full2[convert.vars] <- lapply(full2[convert.vars], function(x) as.numeric(x))
# Get back to train data set
train1 <- full[!is.na(full$Survived),!(names(full) %in% nonvars)]
test1 <- full[is.na(full$Survived),!(names(full) %in% nonvars)]
train2 <- full2[!is.na(full2$Survived),]
test2 <- full2[is.na(full2$Survived),]
# Structure & Correlation matrix
str(train2)
cor(train2)
#INPUT:  train2     CLEAN DATA STORED IN WORKSPACE, ALL NUMERIC
#INPUT:  test2      CLEAN DATA STORED IN WORKSPACE, ALL NUMERIC
source('KaggelSubmission.R', echo=TRUE)
rm(list=ls())
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
# Load necessary packages and ensure they are active
load.lib = c("randomForest","ggplot2","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
###### IMPORT DATA / CLEAN DATA / IMPUTE VARIABLES / CREATE NEW VARIABLES
#INPUT: train.csv   RAW DATA FROM KAGGLE
#INPUT: test.csv    RAW DATA FROM KAGGLE
source('BaseVars.R', echo=TRUE)
#OUTPUT: train      RAW DATA LOADED IN WORKSPACE
#OUTPUT: test       RAW DATA LOADED IN WORKSPACE
#OUTPUT: train1     CLEAN DATA STORED IN WORKSPACE
#OUTPUT: test1      CLEAN DATA STORED IN WORKSPACE
#OUTPUT: train2     CLEAN DATA STORED IN WORKSPACE, ALL NUMERIC
#OUTPUT: test2      CLEAN DATA STORED IN WORKSPACE, ALL NUMERIC
#INPUT:  train2     CLEAN DATA STORED IN WORKSPACE, ALL NUMERIC
#INPUT:  test2      CLEAN DATA STORED IN WORKSPACE, ALL NUMERIC
source('KaggelSubmission.R', echo=TRUE)
#OUTPUT: TitanicLogKaggel     CLEAN DATA STORED IN WORKSPACE, KAGGEL SUBMISSION
#OUTPUT: titanic_output.csv   OUTPUT FILE FOR KAGGEL SUBMISSION
load.lib = c("randomForest","ggplot2","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
source('BaseVars.R', echo=TRUE)
View(full)
source('TitanicR.R', echo=TRUE)
rm(list=ls())
load.lib = c("randomForest","ggplot2","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
source('BaseVars.R', echo=TRUE)
warnings()
levels(train1$AgeBin)
rm(list=ls())
load.lib = c("randomForest","ggplot2","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
source('BaseVars.R', echo=TRUE)
levels(train1$AgeBin)
full$AgeBin[10:35]
#full$AgeBin[10:35]
# l <- levels(full$AgeBin)[-(length(levels(full$AgeBin)))]
# #l
# #    replace <NA> with 'unknown'
#levels(full$AgeBin)<-c(l, 'unknown')
str(full$AgeBin)
summary(full$AgeBin)
max(test2$Age)
max(full$Age)
rm(list=ls())
load.lib = c("randomForest","ggplot2","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
###### IMPORT DATA / CLEAN DATA / IMPUTE VARIABLES / CREATE NEW VARIABLES
#INPUT: train.csv   RAW DATA FROM KAGGLE
#INPUT: test.csv    RAW DATA FROM KAGGLE
source('BaseVars.R', echo=TRUE)
max(train$Age)
max(train2$Age)
max(test2$Age)
max(full2$Age)
load.lib = c("randomForest","ggplot2","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
###### IMPORT DATA / CLEAN DATA / IMPUTE VARIABLES / CREATE NEW VARIABLES
#INPUT: train.csv   RAW DATA FROM KAGGLE
#INPUT: test.csv    RAW DATA FROM KAGGLE
source('BaseVars.R', echo=TRUE)
source('KaggelSubmission.R', echo=TRUE)
View(train2)
names(train2)
load.lib = c("ggplot2","glmnet","ROCR")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
# split the training data into a secondary test (not Kaggle)
set.seed(100) # set seed so that same sample can be reproduced in future
# now selecting 80% of data as sample from total 'n' rows of the data
sample <- sample.int(n=nrow(train2), size=floor(.80*nrow(train2)), replace=FALSE)
# subset the data using the sample integer vector created above
train3 <- train2[sample, ]
test3  <- train2[-sample, ]
# Logistic regression model
TitanicModelFull = glm(Survived ~ ., data = train3, family = binomial(link='logit'))
summary(TitanicModelFull)
# Test predictive capability of full model
fittedresults <- predict(TitanicModelFull, newdata=test3, type='response')
# count any NAs in the fittedresults
sum(is.na(fittedresults))
# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
fittedresults <- ifelse(fittedresults > 0.5, 1, 0)
# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(fittedresults != test$Surviced, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)
# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))
# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(fittedresults != test3$Survived, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)
# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))
TitanicModelRed = glm(Survived ~ Pclass + Sex + SibSp + Parch + Embarked + AgeBin, data = train3, family = binomial(link='logit'))
summary(TitanicModelRed)
# Test predictive capability of full model
fittedresults2 <- predict(TitanicModelRed, newdata=test3, type='response')
# count any NAs in the fittedresults
sum(is.na(fittedresults2))
# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
fittedresults2 <- ifelse(fittedresults2 > 0.5, 1, 0)
# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError2 <- mean(fittedresults2 != test3$Survived, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)
# print the output as 100% - error
print(paste('Accuracy',1-misClasificError2)) # Accuracy = 84.36%
View(train3)
GLMTrain.y <- train3$Survived
GLMTrain.y <- as.factor(as.character(GLMTrain.y))
# create train data set while removing "Survived" from the training data
GLMTrain.x <- train[,!(colnames(train3) == "Survived")]
View(GLMTrain.x)
# create train data set while removing "Survived" from the training data
GLMTrain.x <- train3[,!(colnames(train3) == "Survived")]
summary(GLMTrain.x)
# isolate categorical/factors from the continuous features, create dummy variable matrix for all factors
GLMTrain.xfactors <- model.matrix(GLMTrain.y ~ GLMTrain.x$Pclass + GLMTrain.x$Sex + GLMTrain.x$SibSp + GLMTrain.x$Parch + GLMTrain.x$Embarked + GLMTrain.x$Title + GLMTrain.x$AgeBin)[, -1]
# remove categorical/factors from GLMTrain.x as they will be added back in the form of dummy variable matrix from above
dropcolsGLM <- c("Pclass", "Sex", "SibSp", "Parch", "Embarked", "Title", "AgeBin")
GLMTrain.x <- GLMTrain.x[,!(colnames(GLMTrain.x) %in% dropcolsGLM)]
# combine GLMTrain.x continuous variables with GLMTrain.xfactors dummy variable matrix, then converting whole thing to a matrix for glmnet
GLMTrain.x <- as.matrix(data.frame(GLMTrain.x, GLMTrain.xfactors))
# use glmnet to fit a binomial logistic regression
glmnetfit <- cv.glmnet(GLMTrain.x, GLMTrain.y, family = "binomial", alpha=1)
plot(glmnetfit)
par(mfrow=c(1,1))
plot(glmnetfit)
lambda_lse <- glmnetfit$lambda.1se
coef(glmnetfit, s=lambda_lse)
GLMfittedresults <- predict(glmnetfit, newx=GLMTest.x, type='response')
# prepare the test data in a similar manner for GLMNET usage (create dummy variables, matrix, etc.)
# create test data set while removing "Survived" from the test data
GLMTest.x <- test[,!(colnames(test3) == "Survived")]
# isolate the binary response "Attrition" from the test data
GLMTest.y <- test3$Survived
GLMTest.y <- as.factor(as.character(GLMTest.y))
# isolate categorical/factors from the continuous features, create dummy variable matrix for all factors
GLMTest.xfactors <- model.matrix(GLMTest.y ~ GLMTest.x$Pclass + GLMTest.x$Sex + GLMTest.x$SibSp + GLMTest.x$Parch + GLMTest.x$Embarked + GLMTest.x$Title + GLMTest.x$AgeBin)[, -1]
View(GLMTest.x)
# prepare the test data in a similar manner for GLMNET usage (create dummy variables, matrix, etc.)
# create test data set while removing "Survived" from the test data
GLMTest.x <- test3[,!(colnames(test3) == "Survived")]
# isolate the binary response "Attrition" from the test data
GLMTest.y <- test3$Survived
GLMTest.y <- as.factor(as.character(GLMTest.y))
# isolate categorical/factors from the continuous features, create dummy variable matrix for all factors
GLMTest.xfactors <- model.matrix(GLMTest.y ~ GLMTest.x$Pclass + GLMTest.x$Sex + GLMTest.x$SibSp + GLMTest.x$Parch + GLMTest.x$Embarked + GLMTest.x$Title + GLMTest.x$AgeBin)[, -1]
# remove categorical/factors from GLMTest.x as they will be added back in the form of dummy variable matrix from above
#dropcolsGLM <- c("Pclass", "Sex", "SibSp", "Parch", "Embarked", "Title", "AgeBin") # From Above
GLMTest.x <- GLMTest.x[,!(colnames(GLMTest.x) %in% dropcolsGLM)]
# combine GLMTest.x continuous variables with GLMTest.xfactors dummy variable matrix, then converting whole thing to a matrix for glmnet
GLMTest.x <- as.matrix(data.frame(GLMTest.x, GLMTest.xfactors))
# predict based on the test data, type='response' output probabilities in the form of P(y=1|X)
GLMfittedresults <- predict(glmnetfit, newx=GLMTest.x, type='response')
# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
GLMfittedresults <- ifelse(GLMfittedresults > 0.5, 1, 0)
# calculate the mean of the fitted results that don't equal the observed result - IGNORE NAs
misClasificError <- mean(GLMfittedresults != GLMTest.y, na.rm=TRUE) # this adds up all the instances of misclassification then divides by total (via mean)
# print the output as 100% - error
print(paste('Accuracy',1-misClasificError))
#Create ROC curves
pr <- prediction(fittedresults, test$Survived)
View(test3)
pr <- prediction(fittedresults, test3$Survived)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
#Ref line indicating poor performance, 50/50
abline(a=0, b= 1)
# calculate area under curve (AUC)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
# print AUC onto plot
text(x = .40, y = .6,paste("AUC = ", round(auc,3), sep = ""))
plot(prf)
#Ref line indicating poor performance, 50/50
segments(0, 0, 1,1)
plot(prf, lwd=2, colorize=TRUE)
# Ref line indicating poor performance, 50/50
segments(0, 0,1,1)
# print AUC onto plot
text(x = .40, y = .6,paste("AUC = ", round(auc,3), sep = ""))
load.lib = c("randomForest","ggplot2","VIM","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
install.packages("vcd")
install.packages("laeken")
load.lib = c("randomForest","ggplot2","VIM","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
install.packages("VIM")
load.lib = c("randomForest","ggplot2","VIM","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependences=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
train <- read.csv('Data/train.csv') # Reading from location after clone
Amelia::missmap(train, main="Missing Values in Train Data", col = c("black","light blue"))
test <- read.csv('Data/test.csv') # Reading from location after clone
Amelia::missmap(test, main="Missing Values in Test Data", col = c("black","light blue"))
# Append test to train for data review and cleaning (result column only valid in train)
full <- bind_rows(train, test)
full_aggr = VIM::aggr(full, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(full), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
View(full)
full_aggr = VIM::aggr(full[,6:10], col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(full), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
# Currently 18 levels for Factor title
table(full$Sex, full$Title)
uncommon <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
full$Title[full$Title == 'Mlle']  <- 'Miss'
full$Title[full$Title == 'Ms']  <- 'Miss'
full$Title[full$Title == 'Mme']  <- 'Mrs'
full$Title[full$Title %in% uncommon]  <- 'uncommon'
# Reduced to 5 levels for Factor Title
table(full$Sex, full$Title)
# Load necessary packages and ensure they are active
load.lib = c("randomForest","ggplot2","ggthemes","mice","scales","dplyr","Amelia","ROCR", "boot", "bestglm","corrplot")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependencies=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(corMatrix, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.01, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
# Load train and test csv files from working directory
# Using Amelia package visualize where we need imputation
#setwd("~/Stats2_Proj2/")
par(mfrow=c(1,2))
train <- read.csv('Data/train.csv') # Reading from location after clone
Amelia::missmap(train, main="Missing Values in Train Data", col = c("black","light blue"))
test <- read.csv('Data/test.csv') # Reading from location after clone
Amelia::missmap(test, main="Missing Values in Test Data", col = c("black","light blue"))
par(mfrow=c(1,1))
# Append test to train for data review and cleaning (result column only valid in train)
full <- bind_rows(train, test)
# Review if components of name, specifically title add to prediction
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
# Currently 18 levels for Factor title
table(full$Sex, full$Title)
uncommon <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
# Combine different titles into similar category
# Mlle is french for 'Mademoiselle'
# Mme is French for 'Madame'
# https://en.wikipedia.org/wiki/Mademoiselle_(title)
# https://en.wikipedia.org/wiki/French_honorifics
full$Title[full$Title == 'Mlle']  <- 'Miss'
full$Title[full$Title == 'Ms']  <- 'Miss'
full$Title[full$Title == 'Mme']  <- 'Mrs'
full$Title[full$Title %in% uncommon]  <- 'uncommon'
# Reduced to 5 levels for Factor Title
table(full$Sex, full$Title)
# Handle the null Fare in Test
# The passenger departed from S and had Pclass = 3, lets find an appropriate value
# Replace missing fare value with median fare for class/embarkment
nullFare = full[is.na(full$Fare),] #1044
ggplot(full[full$Pclass == '3' & full$Embarked == 'S', ], aes(x = Fare)) +
geom_density(fill = '#99d6ff', alpha=0.4) +
geom_vline(aes(xintercept=median(Fare, na.rm=T)),colour='red', linetype='dashed', lwd=1) +
scale_x_continuous(breaks = seq(0,70,10), labels=dollar_format()) +
ggtitle("Density of Ticket Fare for Embarked = S and Pclass = 3",
subtitle = paste0("Median Fare: $",
median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm=T))) +
theme_few()
full$Fare[1044] <- median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE) # $8.05
# Using mice package impute values for Age that are missing
sum(is.na(train$Age)) # 177 missing values
sum(is.na(full$Age))  # 263 missing values in both train and test
# Set a random seed
set.seed(129)
# Perform mice imputation, excluding certain less-than-useful variables:
mice_mod <- mice(full[, !names(full) %in% c('PassengerId','Name','Ticket','Cabin','Survived')], method='rf')
# Save the complete output
mice_output <- complete(mice_mod)
# Plot age distributions of raw data against imputed from mice package
par(mfrow=c(1,2))
hist(full$Age, freq=F, main='Age: Original Data', col='lightblue', ylim=c(0,0.04))
hist(mice_output$Age, freq=F, main='Age: MICE Imputation Output', col='lightblue', ylim=c(0,0.04))
# Replace Age variable from the mice model
full$Age <- mice_output$Age
# Show new number of missing Age values is now 0
sum(is.na(full$Age))
# Create Age as a categorical variable
full$AgeBin <- cut(full$Age, seq(0, 90, by=3))
levels(full$AgeBin) <- c(rep("6 or less", 2), rep("(7 - 63]", 19), rep("Over 63", 9))
#full$AgeBin <- cut(full$Age, seq(0, 80, by=10))
#full$AgeBin[10:35]
# l <- levels(full$AgeBin)[-(length(levels(full$AgeBin)))]
# #l
# #    replace <NA> with 'unknown'
#levels(full$AgeBin)<-c(l, 'unknown')
summary(full$AgeBin)
# Create a family = siblings + parents/children
# -Possibly for dimension reducing
full$Family = full$Parch + full$SibSp
# Make variables factors into factors
factor_vars <- c('PassengerId','Pclass','Sex','Embarked','Title', 'AgeBin')
full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))
# Create DF of independent/dependent variables
nonvars = c("PassengerId","Name","Ticket","Cabin")
full2 = full[,!(names(full) %in% nonvars)]
str(full2)
convert.vars <- c('Pclass','Sex','Embarked','Title', 'AgeBin')
full2[convert.vars] <- lapply(full2[convert.vars], function(x) as.numeric(x))
# Get back to train data set
train1 <- full[!is.na(full$Survived),!(names(full) %in% nonvars)]
test1 <- full[is.na(full$Survived),!(names(full) %in% nonvars)]
train2 <- full2[!is.na(full2$Survived),]
test2 <- full2[is.na(full2$Survived),]
# Structure & Correlation matrix
str(train2)
corMatrix = cor(train2)
corMatrix
write.csv(corMatrix, "CorrelationMatrix.csv")
par(mfrow=c(1,1))
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(corMatrix, method="color", col=col(200),
type="upper", order="hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.01, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
corrplot(corMatrix, method="number")
corrplot(corMatrix, method="number", type = "upper")
corrplot(corMatrix, method="number", type = "upper",
addshade = "all", shade.lwd = 1, shade.col = "white")
corrplot(corMatrix, method="number", type = "upper",
addshade = "all", shade.lwd = 1, shade.col = "gray")
corrplot(corMatrix, method="number", type = "upper",
addshade = "all", shade.lwd = 4, shade.col = "gray")
corrplot(corMatrix, method="color", type = "upper",
addshade = "all", shade.lwd = 1, shade.col = "gray")
corrplot(corMatrix, order = "AOE", type = "upper", tl.pos = "d")
corrplot(corMatrix, add = TRUE, type = "lower", method = "number", order = "AOE",
diag = FALSE, tl.pos = "n", cl.pos = "n")
corrplot(corMatrix, add = TRUE, type = "lower", method = "number", order = "original",
diag = FALSE, tl.pos = "n", cl.pos = "n")
corrplot(corMatrix, add = TRUE, type = "lower", method = "number", order = "original",
diag = TRUE, tl.pos = "n", cl.pos = "n")
corrplot(corMatrix, type = "upper", tl.pos = "d")
corrplot(corMatrix, type = "upper")
corrplot(corMatrix, add = TRUE, type = "lower", method = "number", order = "original",
diag = TRUE, tl.pos = "td") #, cl.pos = "n")
corrplot(corMatrix, add = TRUE, type = "upper", method = "number", order = "original",
diag = TRUE, tl.pos = "td") #, cl.pos = "n")
corrplot(corMatrix, add = TRUE, type = "upper", method = "number", order = "original",
diag = FALSE, tl.pos = "td") #, cl.pos = "n")
corrplot(corMatrix, type = "upper", method = "number", order = "original",
diag = FALSE, tl.pos = "td") #, cl.pos = "n")
corrplot(corMatrix, method="color",
type="upper", order="hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
# Combine with significance
p.mat = p.mat, sig.level = 0.01, insig = "blank",
# hide correlation coefficient on the principal diagonal
diag=FALSE
)
cor.mtest <- function(mat, ...) {
mat <- as.matrix(mat)
n <- ncol(mat)
p.mat<- matrix(NA, n, n)
diag(p.mat) <- 0
for (i in 1:(n - 1)) {
for (j in (i + 1):n) {
tmp <- cor.test(mat[, i], mat[, j], ...)
p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
}
}
colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
p.mat
}
p.mat <- cor.mtest(train2)
head(p.mat[, 1:5])
corrplot(corMatrix, method="color",
type="upper", order="hclust",
addCoef.col = "black", # Add coefficient of correlation
tl.col="black", tl.srt=45, #Text label color and rotation
p.mat = p.mat, sig.level = 0.01, insig = "blank",
diag=FALSE
)
# Load necessary packages and ensure they are active
load.lib = c("randomForest","ggplot2","ggthemes","mice","scales","dplyr","Amelia","ROCR","glmnet","boot","bestglm","corrplot")
install.lib = load.lib[!load.lib %in% installed.packages()]
for(lib in install.lib){
install.packages(lib,dependencies=TRUE)
}
suppressMessages(sapply(load.lib,require,character=TRUE))
# split the training data into a secondary test (not Kaggel)
set.seed(100) # set seed so that same sample can be reproduced in future
# now selecting 80% of data as sample from total 'n' rows of the data
sample <- sample.int(n=nrow(train2), size=floor(.80*nrow(train2)), replace=FALSE)
# subset the data using the sample integer vector created above
train3 <- train2[sample, ]
test3  <- train2[-sample, ]
# Logistic regression full model
TitanicModelFull = glm(Survived ~ ., data = train3, family = binomial(link='logit'))
summary(TitanicModelFull)
View(train3)
fittedresults2 <- predict(TitanicModelRed, newdata=test3, type='response')
fittedresults <- predict(TitanicModelFull, newdata=test3, type='response')
# count any NAs in the fittedresults
sum(is.na(fittedresults))
# if P(y=1|X) > 0.5 then y = 1 otherwise y=0
fittedresults <- ifelse(fittedresults > 0.5, 1, 0)
